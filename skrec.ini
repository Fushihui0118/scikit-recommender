[skrec]
recommender = SRGNN

;dataset
data_dir = dataset/Beauty_loo_u5_i5
; file_column = UI, UIR, UIT, UIRT
file_column = UIRT
sep = ','
;--------------------
;GPU
gpu_id = 0
;--------------------
;test
;metric = ("Precision", "Recall", "MAP", "NDCG", "MRR")
metric = ("Recall", "NDCG")
top_k = (10,20,30,40,50)
test_thread = 4
; large test_batch_size might cause GPU memory-consuming,
; especially dataset is large
test_batch_size = 64
seed = 2021

[BPRMF]
lr = 1e-3
reg = 1e-3
n_dim = 64
batch_size = 1024
epochs = 1000
early_stop = 200

[BERT4Rec]
; bert model
att_drop = 0.2
h_drop = 0.5
h_size = 32
att_heads = 2
; do not tune
init_range = 0.02
h_act = gelu
n_layers = 2
;--------------------
;train
lr = 1e-4
batch_size = 256
save_ckpt_epoch = 10
init_ckpt = None
epochs = 3000
early_stop = 50
verbose = 10
;--------------------
;prepare dataset
max_seq_len = 5
masked_lm_prob = 0.2
sliding_step = 1
dupe_factor = 10
pool_size = 10

[AOBPR]
lr = 0.01
reg = 0.05
embed_size = 16
epochs = 2000
early_stop = 100
alpha = 1682

[LightGCN]
lr = 1e-3
reg = 1e-4
embed_size = 64
n_layers = 3
batch_size = 1024
;adj_type = plain, norm, gcmc, pre
adj_type = pre
early_stop = 100
epochs = 1000

[SASRec]
lr = 0.001
l2_emb = 0.0
hidden_units = 64
dropout_rate = 0.5
max_len = 50
num_blocks = 2
num_heads = 1
batch_size = 128
early_stop = 100
epochs = 1000

[HGN]
lr=1e-3
reg = 1e-3
seq_L = 5
seq_T = 3
embed_size = 64
batch_size = 1024
early_stop = 1000
epochs = 1000

[TransRec]
lr = 0.001
reg = 0.0
embed_size = 64
batch_size = 1024
epochs = 1000
early_stop = 200

[SRGNN]
lr = 0.001
l2_reg = 1e-5
hidden_size = 64
lr_dc = 0.1
lr_dc_step = 3
step = 1
nonhybrid = False
; max_seq_len is used to save gpu memory by limiting the max length of item sequence
max_seq_len = 200
batch_size = 256
epochs = 500
early_stop = 100
